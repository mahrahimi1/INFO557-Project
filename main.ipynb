{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import keras\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, Dropout, Input, MaxPooling1D, Reshape\n",
    "from keras.layers import Dense, SimpleRNN, GRU, LSTM, Conv2D, MaxPooling2D, Flatten, Embedding, Conv1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import EarlyStopping, History\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tweets_and_labels(file_name):\n",
    "    f = open(file_name, encoding=\"utf8\")\n",
    "    isFirstLine = True\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    tweet_ids = []\n",
    "    for line in f:\n",
    "        if (isFirstLine):\n",
    "            isFirstLine = False\n",
    "            continue\n",
    "        values = line.strip().split(\"\\t\")\n",
    "        \n",
    "        tweets.append(values[1])\n",
    "        \n",
    "        tweet_ids.append(values[0])\n",
    "    \n",
    "        label = []\n",
    "        for i in range(2,13):\n",
    "            try:\n",
    "                label.append(int(values[i]))\n",
    "            except:\n",
    "                label.append(0)\n",
    "        labels.append(label)\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return tweet_ids, tweets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet_ids, train_tweets , train_labels = create_tweets_and_labels('./2018-E-c-En-train.txt')\n",
    "dev_tweet_ids, dev_tweets, dev_labels = create_tweets_and_labels('./2018-E-c-En-dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(train_tweets)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "encoded_train_tweets = t.texts_to_sequences(train_tweets)\n",
    "encoded_dev_tweets = t.texts_to_sequences(dev_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for encoded_tweet in encoded_train_tweets:\n",
    "    if len(encoded_tweet) > max_len:\n",
    "        max_len = len(encoded_tweet)\n",
    "\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad documents\n",
    "padded_train_tweets = pad_sequences(encoded_train_tweets, maxlen=max_len, padding='post')\n",
    "padded_dev_tweets = pad_sequences(encoded_dev_tweets, maxlen=max_len, padding='post')\n",
    "\n",
    "x_train = padded_train_tweets\n",
    "y_train = train_labels\n",
    "\n",
    "x_dev = padded_dev_tweets\n",
    "y_dev = dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embedding_file = './glove/glove.840B.300d.txt'\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open(embedding_file, encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        print(values)\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(vocab_size, EMBEDDING_DIM,weights=[embedding_matrix]\n",
    "                       ,input_shape=(max_len,),trainable=False,name='Embedding')(x)\n",
    "\n",
    "g = 100\n",
    "gru1_layer = Bidirectional(GRU(g, return_sequences=True), merge_mode='concat')(embedding_layer)\n",
    "\n",
    "e = g*2\n",
    "reshape_layer = Reshape((max_len , e , 1) , name='Reshape_Embedding')(gru1_layer)\n",
    "\n",
    "one_gram_conv = Conv2D(200, kernel_size=(1, e), activation='relu' , use_bias=True\n",
    "                       , name='1Gram_Conv')(reshape_layer)\n",
    "one_gram_maxpool = MaxPooling2D(pool_size=(max_len - 1 + 1, 1) \n",
    "                                , name='1Gram_Maxpool')(one_gram_conv)\n",
    "one_gram_flatten = Flatten(name='1Gram_TimeDistributed_Flatten')(one_gram_maxpool)\n",
    "\n",
    "two_gram_conv = Conv2D(200, kernel_size=(2, e), activation='relu' , use_bias=True \n",
    "                                , name='2Gram_Conv')(reshape_layer)\n",
    "two_gram_maxpool = MaxPooling2D(pool_size=(max_len - 2 + 1, 1) \n",
    "                                   , name='2Gram_Maxpool')(two_gram_conv)\n",
    "two_gram_flatten = Flatten(name='2Gram_Flatten')(two_gram_maxpool)\n",
    "\n",
    "three_gram_conv = Conv2D(200, kernel_size=(3, e), activation='relu' , use_bias=True \n",
    "                                  , name='3Gram_Conv')(reshape_layer)\n",
    "three_gram_maxpool = MaxPooling2D(pool_size=(max_len - 3 + 1, 1) \n",
    "                                , name='3Gram_Maxpool')(three_gram_conv)\n",
    "three_gram_flatten = Flatten(name='3Gram_Flatten')(three_gram_maxpool)\n",
    "\n",
    "merge = concatenate([one_gram_flatten, two_gram_flatten , three_gram_flatten] , name='Merge_n-grams')\n",
    "\n",
    "fully_connected = Dense(20,activation='relu',name='Fully_Connected_Layer')(merge)\n",
    "\n",
    "output_layer = Dense(11, activation='sigmoid', name='Output_Layer')(fully_connected)\n",
    "\n",
    "output = output_layer\n",
    "model = Model(inputs=x , outputs=output)\n",
    "\n",
    "#a = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 5\n",
    "callbacks=[EarlyStopping(monitor='val_loss', patience=patience, verbose=1, restore_best_weights=True)]\n",
    "\n",
    "history = model.fit(x_train,y_train,\n",
    "                    validation_data=[x_dev,y_dev],\n",
    "                    epochs=50, \n",
    "                    batch_size=100,\n",
    "                    verbose=1\n",
    "                    ,callbacks=callbacks\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_dev, y_dev, verbose=0)\n",
    "print('\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predic_probs = model.predict(padded_dev_tweets)\n",
    "dev_predic_classes = zeros((padded_dev_tweets.shape[0],11),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4\n",
    "for i in range(dev_predic_probs.shape[0]):\n",
    "    for j in range(11):\n",
    "        if (dev_predic_probs[i][j] >= threshold):\n",
    "            dev_predic_classes[i][j] = 1\n",
    "        else:\n",
    "            dev_predic_classes[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "sklearn.metrics.jaccard_similarity_score(dev_labels , dev_predic_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dev predictions file\n",
    "f = open(\"E-C_en_pred.txt\", \"r+\", encoding=\"utf8\")\n",
    "f.write(\"ID\\tTweet\\tanger\\tanticipation\\tdisgust\\tfear\\tjoy\\tlove\\toptimism\\tpessimism\\tsadness\\tsurprise\\ttrust\\n\")\n",
    "\n",
    "for i in range(dev_predic_classes.shape[0]):\n",
    "    f.write(dev_tweet_ids[i] + \"\\t\" + dev_tweets[i] + \"\\t\") \n",
    "    for j in range(10):\n",
    "        f.write((dev_predic_classes[i][j]).__str__() + \"\\t\")\n",
    "    f.write((dev_predic_classes[i][10]).__str__() + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions for test set\n",
    "test_tweet_ids, test_tweets, test_labels = create_tweets_and_labels('./2018-E-c-En-test.txt')\n",
    "\n",
    "encoded_test_tweets = t.texts_to_sequences(test_tweets)\n",
    "\n",
    "padded_test_tweets = pad_sequences(encoded_test_tweets, maxlen=max_len, padding='post')\n",
    "\n",
    "test_predic_probs = model.predict(padded_test_tweets)\n",
    "test_predic_classes = zeros((padded_test_tweets.shape[0],11),dtype=int)\n",
    "\n",
    "threshold = 0.4\n",
    "for i in range(test_predic_probs.shape[0]):\n",
    "    for j in range(11):\n",
    "        if (test_predic_probs[i][j] >= threshold):\n",
    "            test_predic_classes[i][j] = 1\n",
    "        else:\n",
    "            test_predic_classes[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test predictions file\n",
    "f = open(\"E-C_en_pred.txt\", \"r+\", encoding=\"utf8\")\n",
    "f.write(\"ID\\tTweet\\tanger\\tanticipation\\tdisgust\\tfear\\tjoy\\tlove\\toptimism\\tpessimism\\tsadness\\tsurprise\\ttrust\\n\")\n",
    "\n",
    "for i in range(test_predic_classes.shape[0]):\n",
    "    f.write(test_tweet_ids[i] + \"\\t\" + test_tweets[i] + \"\\t\") \n",
    "    for j in range(10):\n",
    "        f.write((test_predic_classes[i][j]).__str__() + \"\\t\")\n",
    "    f.write((test_predic_classes[i][10]).__str__() + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
